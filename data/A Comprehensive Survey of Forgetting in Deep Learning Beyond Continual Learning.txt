Abstract

Forgetting refers to the loss or deterioration of previously acquired knowledge. While existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new task, managing task interference with conflicting goals, and preventing privacy leakage, etc.
Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we present a more nuanced understanding of this phenomenon and highlight its potential advantages.
Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.
A comprehensive list of papers about forgetting in various research fields is available at 
https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning
.

1 
Introduction

Forgetting 
[
1
]
 refers to the phenomenon where previously acquired information or knowledge in a machine learning system degrades over time. In the early days of neural networks, the focus was primarily on training models on static datasets. Forgetting was not a significant concern since the models were trained and evaluated on fixed datasets.
The concept of catastrophic forgetting was first formally introduced by McCloskey and Cohen 
[
1
]
. They demonstrated that neural networks when trained sequentially on different tasks, tend to forget previously learned tasks when new tasks are learned.
Later, addressing the issue of forgetting was formalized as continual learning (CL). Nowadays, forgetting has garnered significant attention not only within the CL domain but also in the broader machine learning community, which has evolved into a fundamental problem in the field of machine learning.

Existing surveys on forgetting have primarily focused on CL 
[
2
, 
3
, 
4
, 
5
, 
6
, 
7
, 
8
, 
9
]
. However, these surveys tend to concentrate solely on the harmful effects of forgetting and lack a comprehensive discussion on the topic.
In contrast, we highlight the dual nature of forgetting as a double-edged sword, emphasizing both its benefits and harms. Additionally, our survey extends beyond the scope of CL and covers the forgetting issue in various other domains, including foundation models, domain adaptation, meta-learning, test-time adaptation, generative models, reinforcement learning and federated learning. By doing so, we offer a comprehensive examination of forgetting that encompasses a broader range of contexts and applications.

TABLE I: 
Harmful Forgetting: Comparisons among different problem settings. 

Problem Setting

Goal

Source of Forgetting

Continual Learning

learn non-stationary data distribution without forgetting previous knowledge

data-distribution shift during training

Foundation Model

unsupervised learning on large-scale unlabeled data

data-distribution shift in pre-training, fine-tuning

Domain Adaptation

adapt to target domain while maintaining performance on source domain

target domain sequentially shift over time

Test-time Adaptation

mitigate the distribution gap between training and testing

adaptation to the test data distribution during testing

Meta Learning

learn adaptable knowledge to new tasks

incrementally meta-learn new classes / task-distribution shift

Generative Model

learn a generator to approximate real data distribution

generator shift / data-distribution shift

Reinforcement Learning

maximize accumulate rewards

state, action, reward and state transition dynamics shift

Federated Learning

decentralized training without sharing data

model average; non-i.i.d data; data-distribution shift

TABLE II: 
Beneficial Forgetting: Comparisons among different problem settings.

Problem Setting

Goal

Mitigate Overfitting

mitigate memorization of training data through selective forgetting

Debias and Forget Irrelevant Information

forget biased information to achieve better performance or remove irrelevant information to learn new tasks

Machine Unlearning

forget some specified training data to protect user privacy

In this survey, we classify forgetting in machine learning into: harmful forgetting and beneficial forgetting, based on the specific application scenarios. Harmful forgetting occurs when we desire the machine learning model to retain previously learned knowledge while adapting to new tasks, domains, or environments. In such scenarios, it is crucial to prevent knowledge forgetting. Conversely, there are many cases where beneficial forgetting becomes necessary. For example: (1) Overfitting to the training data hinders generalization. (2) Irrelevant and noisy information impedes the model’s ability to effectively learn new knowledge. (3) Pre-trained model contains private information that could potentially lead to privacy leakage. In these situations, forgetting becomes desirable as it serves several important purposes. Firstly, forgetting can mitigate overfitting, as it allows the model to forget irrelevant details and focus on the most pertinent patterns in the training data. Additionally, by discarding unnecessary information, forgetting facilitates the learning of new knowledge, as the model can make better use of its capacity to acquire and adapt to novel information. Lastly, forgetting helps protect privacy by discarding sensitive user information, ensuring that such data is not retained in the model’s memory.

1.1 
Harmful Forgetting

Harmful forgetting has been observed not only in CL but also in various other research areas, including foundation model, domain adaptation, meta-learning, test-time adaptation, generative model, reinforcement learning and federated learning. While existing surveys have predominantly focused on forgetting in CL, this survey aims to fill the gap by providing an overview of forgetting across various learning scenarios.

Forgetting in these research fields can be attributed to various factors. In continual learning, forgetting occurs due to the shift in data distribution across different tasks. In meta-learning, forgetting is a consequence of the shift in task distribution. In federated learning, forgetting is caused by the heterogeneity of data distribution among different clients, commonly known as client drift. In domain adaptation, forgetting happens because of domain shift. In test-time adaptation, forgetting is a result of adapting to the test data distribution during testing. In generative models, forgetting occurs due to the shift in the generator over time or when learning non-stationary data distribution. In reinforcement learning, forgetting can occur as a result of shifts in state, action, reward, and state transition dynamics over time. These changes in the underlying environment can lead to the loss or alteration of previously learned knowledge in reinforcement learning. In foundation models, forgetting can be attributed to: fine-tuning forgetting, incremental streaming data pre-training, and the utilization of foundation models for downstream CL tasks.

To facilitate comparisons of various settings related to forgetting, we present a comprehensive analysis of harmful forgetting in Table 
I
.

Harmful Forgetting Definition
. We denote the performance on the test set 
X
t
subscript
𝑋
𝑡
X_{t}
italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 when learning task/domain 
t
𝑡
t
italic_t
 with parameters 
𝜽
t
subscript
𝜽
𝑡
{\bm{\theta}}_{t}
bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 as 
ℒ
⁢
(
𝜽
t
,
X
t
)
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{t},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
. We denote the performance on the test set 
X
t
subscript
𝑋
𝑡
X_{t}
italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 after learning the last task/domain 
T
𝑇
T
italic_T
 with parameters 
𝜽
T
subscript
𝜽
𝑇
{\bm{\theta}}_{T}
bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT
 as 
ℒ
⁢
(
𝜽
T
,
X
t
)
ℒ
subscript
𝜽
𝑇
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{T},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
. Forgetting after learning task/domain 
T
𝑇
T
italic_T
 can then be defined as follows:

Definition 1.1
 (Forgetting)

F
=
1
T
−
1
⁢
∑
t
=
1
T
(
ℒ
⁢
(
𝜽
T
,
X
t
)
−
ℒ
⁢
(
𝜽
t
,
X
t
)
)
𝐹
1
𝑇
1
superscript
subscript
𝑡
1
𝑇
ℒ
subscript
𝜽
𝑇
subscript
𝑋
𝑡
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
F=\frac{1}{T-1}\sum_{t=1}^{T}({\mathcal{L}}({\bm{\theta}}_{T},X_{t})-{\mathcal%
{L}}({\bm{\theta}}_{t},X_{t}))
italic_F = divide start_ARG 1 end_ARG start_ARG italic_T - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )

(1)

This definition covers various learning scenarios in different settings. For example, for continual learning, 
ℒ
⁢
(
𝜽
t
,
X
t
)
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{t},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 denotes the test set performance on task 
t
𝑡
t
italic_t
. For reinforcement learning, 
ℒ
⁢
(
𝜽
t
,
X
t
)
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{t},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 denotes the cumulative reward/average reward/discounted reward on task 
t
𝑡
t
italic_t
. For meta-learning, 
ℒ
⁢
(
𝜽
t
,
X
t
)
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{t},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 denotes the meta test set accuracy on task distribution 
t
𝑡
t
italic_t
. For generative model, 
ℒ
⁢
(
𝜽
t
,
X
t
)
ℒ
subscript
𝜽
𝑡
subscript
𝑋
𝑡
{\mathcal{L}}({\bm{\theta}}_{t},X_{t})
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 denotes the Frechet Inception Distance (FID) or Inception Score (IS).

1.2 
Beneficial Forgetting

While the prevailing belief in most existing works is that forgetting is harmful, we have come to recognize that forgetting is a double-edged sword. There are many instances where it is advantageous to forget certain knowledge. For example: (1) selective forgetting could help mitigate overfitting; (2) to enhance model generalization or facilitate learning of new tasks/knowledge, it is imperative to eliminate biased or irrelevant information from previously learned knowledge; and (3) machine unlearning, which prevents data privacy leakage.

First, overfitting has remained a fundamental challenge in machine learning, as it arises when a model excessively memorizes the training data but struggles to generalize effectively to new, unseen test data. To improve generalization, it is crucial for the model to avoid the mere memorization of training data and instead should prioritize learning the true underlying relationship between the input data and corresponding labels.
One important technique to enhance generalization is selective forgetting. By selectively discarding irrelevant or noisy information learned from training data, the model can focus on the most pertinent patterns and features, leading to improved generalization performance on unseen data.

Second, when learning new tasks or knowledge, previously acquired knowledge may not always be helpful for improving learning on new information. When a model contains outdated or unrelated knowledge, it can hinder its ability to effectively learn and generalize from new data. In such situations, it is necessary to discard irrelevant information from the model’s memory. By freeing up capacity within the model, it becomes more receptive and adaptive to acquiring new knowledge.
The process of discarding irrelevant information is crucial for preventing interference between old and new knowledge.

Lastly, model users may request the removal of their training data from both the database and the pre-trained model, exercising their Right to Be Forgotten 
[
10
]
. To address this, researchers have developed machine unlearning, which allows models to intentionally forget unwanted private data. Additionally, some privacy attacks exploit the model’s tendency to memorize data to extract private information. Membership inference attacks 
[
11
]
 can identify whether a specific data point was part of the training data for a pre-trained model. Intentional forgetting of private data helps protect privacy and prevent information leakage in such cases.

To facilitate comparisons, we also provide a comparative analysis in Table 
II
 for beneficial forgetting, encompassing the above mentioned diverse settings for reference.

1.3 
Challenges in Addressing Forgetting

Addressing forgetting faces numerous challenges that vary across different research fields. These challenges include:

Data Availability
:
Data availability is a significant challenge for addressing forgetting in various scenarios. Limited access to previous task data, due to storage constraints or privacy concerns, complicates continual learning, meta-learning, domain adaptation, generative models, and reinforcement learning. Additionally, some scenarios, like federated learning, prohibit using raw data, as only the model parameters are shared with a central server.

Resource Constraints
: Resource-limited environments, such as those with constraints on memory and computation, present challenges in effectively addressing forgetting. In online continual learning and meta-learning, where data or tasks are typically processed only once, these challenges are particularly pronounced.
Furthermore, online learning often operates in resource-constrained environments with limited memory or computation capabilities. These constraints pose additional hurdles for addressing forgetting.

Adaption to New Environments/Distribution
: In continual learning, foundation models, reinforcement learning, domain adaptation, test-time adaptation, meta-learning, and generative models, the target environment or data distribution can change over time. The learning agent must adapt to new scenarios, which can happen during training or testing. However, the agent often forgets previously acquired knowledge or loses performance on earlier tasks due to the data distribution shift.

Task Interference/Inconsistency
:
Conflicting goals among different tasks can cause task interference, making it hard to prevent forgetting in continual learning and federated learning. In continual learning, sequential tasks may conflict, making it difficult for the network to balance performance across multiple tasks and exacerbating forgetting. In federated learning, models trained on different clients can show inconsistencies 
[
12
]
 due to heterogeneous data distributions, leading to client interference and further worsening the forgetting problem.

Privacy-Leakage Prevention
:
In some cases, retaining old knowledge can raise privacy concerns by unintentionally exposing private information. To address these risks and prevent the disclosure of sensitive data, the focus should be on forgetting or erasing training data traces rather than memorizing them. This challenge is central to machine unlearning, which aims to effectively remove training data traces from machine learning models 
[
13
]
.

1.4 
Survey Scope, Contributions and Organization

Survey Scope
.
Our main objective is to give a comprehensive overview of forgetting in key research areas where it is significant. By exploring these fields, we aim to highlight the existence and impact of forgetting in these domains.

Our contributions can be summarized into three fold:

•

We provide a more systematic survey on CL compared to existing surveys. Our survey includes a more systematic categorization of CL problem settings and methods.

•

In addition to CL, our survey extends its scope to encompass forgetting in other research fields. This broader coverage provides a comprehensive understanding of forgetting across various research fields.

•

Our survey, in contrast to existing surveys on CL, reveals that forgetting can be viewed as a double-edged sword. We emphasize that forgetting can also have beneficial implications in privacy-preserving scenarios.

Organization
.
The structure of this paper is as follows.
In Sections 
2
-
8
, we provide a comprehensive survey on harmful forgetting in continual learning, foundation model, domain adaptation, test-time adaptation, meta-learning, generative model, reinforcement learning, and federated learning. Each section explores the occurrence and impact of forgetting within these specific fields.
In Section 
9
, we delve into the concept of beneficial forgetting. This section highlights the positive aspects of forgetting in specific learning scenarios. In Section 
10
, we present the current research trends and offer insights into the potential future developments.

2 
Forgetting in Continual Learning

TABLE III: 
Content outline in CL. Based on different problem setting categorization criteria, the CL setting can be classified into various scenarios, as presented in the following table:

Section

Problem Setting

Categorization Criterion

Section 
2.1

Task-aware and Task-free CL

whether explicit task splits/information are available or not during training

Section 
2.2

Online CL

the model processes the data in a single pass or multiple passes

Section 
2.3

Semi-supervised, Few-shot and Unsupervised CL

the amount of labeled data used in CL

The goal of continual learning (CL) is to learn on a sequence of tasks 
𝒯
1
,
𝒯
2
,
⋯
,
𝒯
N
subscript
𝒯
1
subscript
𝒯
2
⋯
subscript
𝒯
𝑁
{\mathcal{T}}_{1},{\mathcal{T}}_{2},\cdots,{\mathcal{T}}_{N}
caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , caligraphic_T start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT
 without forgetting the knowledge on previous tasks. It can be formulated with the following optimization objective.
Suppose when learning task 
t
𝑡
t
italic_t
, the goal is to minimize the risk on all the seen tasks so far, i.e.,

ℒ
⁢
(
𝜽
t
)
=
∑
t
=
1
N
𝔼
(
𝒙
,
y
)
∼
𝒟
𝒯
t
⁢
ℒ
𝜽
t
⁢
(
𝒙
,
y
)
,
ℒ
subscript
𝜽
𝑡
superscript
subscript
𝑡
1
𝑁
subscript
𝔼
similar-to
𝒙
𝑦
subscript
𝒟
subscript
𝒯
𝑡
subscript
ℒ
subscript
𝜽
𝑡
𝒙
𝑦
{\mathcal{L}}({\bm{\theta}}_{t})=\sum_{t=1}^{N}{\mathbb{E}}_{({\bm{x}},y)\sim{%
\mathcal{D}}_{{\mathcal{T}}_{t}}}{\mathcal{L}}_{{\bm{\theta}}_{t}}({\bm{x}},y),
caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT ( bold_italic_x , italic_y ) ∼ caligraphic_D start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x , italic_y ) ,

(2)

where 
𝔼
𝔼
{\mathbb{E}}
blackboard_E
 denotes expectation, 
𝜽
t
subscript
𝜽
𝑡
{\bm{\theta}}_{t}
bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 are parameters when learning task 
t
𝑡
t
italic_t
, and 
𝒟
𝒯
t
subscript
𝒟
subscript
𝒯
𝑡
{\mathcal{D}}_{{\mathcal{T}}_{t}}
caligraphic_D start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT
 represents the training data of task 
t
𝑡
t
italic_t
.

The CL problem can be categorized in several different ways. Firstly, according to whether explicit task splits/information are available or not during training stage, CL can be divided into 
task-aware (task-based) and task-free (task-agnostic)
 scenarios 
[
14
]
. Task-aware CL can be further classified into task/domain/class incremental learning 
[
3
]
, depending on whether the task ID is known during testing stage. Among them, task-incremental learning knows the task ID during testing, while domain-incremental learning and class-incremental learning do not know the task ID during the testing phase. In particular, the label space of domain-incremental learning is the same, while other settings have independent label spaces. Addressing forgetting in task-aware CL is relatively straightforward due to the availability of task information. With knowledge of the specific tasks involved, CL learner can utilize task-specific cues or labels to guide its learning process and manage forgetting.
However, addressing forgetting in task-free CL is more challenging since there are no explicit task splits or task-specific information available. As a result, the learning system must autonomously identify and adapt to changes or shifts in the data distribution without any task-specific cues or labels. This requires the development of robust and adaptive mechanisms that can detect and respond to changes in the data distribution.

Secondly, depending on whether the model processes the data in a single pass or multiple passes, CL can be categorized as 
online and offline CL
. Offline CL has been extensively studied due to its availability of abundant computing and storage resources. However, online CL presents unique challenges. In online CL, the agent has limited access to past data and experiences, which restricts the opportunities to revisit and reinforce previously learned tasks.
Furthermore, online learning often operates in resource-constrained environments with limited memory or processing capabilities. These resource limitations pose additional hurdles for addressing forgetting in online CL.

Lastly, according to the amount of labeled data used in CL, they could be categorized into 
supervised, semi-supervised, few-shot, and unsupervised CL
.
Supervised CL is generally considered the easiest case since the availability of labeled data provides clear task boundaries and evaluation signals. However, challenges arise in other forms of CL.
For semi-supervised CL: the challenge lies in selecting useful knowledge from unlabeled data to mitigate forgetting. Not all unlabeled data may be beneficial for addressing forgetting, making the selection process challenging.
In few-shot CL: the scarcity of labeled data requires the learning agent to effectively utilize the available information to minimize forgetting and adapt to new tasks.
In unsupervised CL: unsupervised CL is the most challenging due to the absence of explicit task boundaries. Defining when a new task begins and differentiating it from previous tasks becomes difficult.
Furthermore, the absence of labeled data in unsupervised CL results in a scarcity of feedback and evaluation signals for measuring forgetting.

It is important to note that the terms CL and incremental learning (IL) are often used interchangeably when addressing learning from non-stationary data distributions, as described in 
[
3
]
. The key objective of CL and IL is to enable models to learn continuously by updating in stages as new data becomes available, ensuring that they can acquire new knowledge while retaining previously learned information. Traditional online learning (OL), by contrast, represents a special case of IL, where the model processes data streams in real time. In OL, the model is updated immediately upon receiving new data, typically handling one sample (or a small batch) at a time from a stationary data distribution/single task. The primary goal of OL is efficient learning rather than mitigating forgetting. This contrasts with CL and IL, where models are updated across multiple epochs and often adapt to changing, non-stationary data distributions.

Below, we present the details of each CL problem setting and its corresponding related works. To make content organization clear, we provide a Table 
III
 to summarize the problem setting categorization in the following sections.

2.1 
Task-Aware and Task-Free CL

2.1.1 
Task-aware CL

Task-aware CL focuses on addressing scenarios where explicit task definitions, such as task IDs, are available during the CL process. The three most common CL scenarios within task-aware settings are task-incremental learning, domain-incremental learning, and class-incremental learning 
[
3
]
.
In domain-incremental learning, tasks sequentially arrive with the same label space but different input data distributions. This means that the tasks share a common set of labels or categories, but the distribution of the input data may vary across tasks.
Task-incremental learning refers to the scenario where tasks arrive sequentially, and each task has its own disjoint label space. During testing, the presence of task IDs allows the model to identify the specific task at hand.
Class-incremental learning does not provide task IDs during testing. Instead, the model needs to incrementally learn new classes without forgetting previously learned classes.

Problem Setup:
 We consider the standard CL problem of learning a sequence of 
N
𝑁
N
italic_N
 tasks denoted as 
𝒟
t
⁢
r
=
{
𝒟
1
t
⁢
r
,
𝒟
2
t
⁢
r
,
⋯
,
𝒟
N
t
⁢
r
}
superscript
𝒟
𝑡
𝑟
superscript
subscript
𝒟
1
𝑡
𝑟
superscript
subscript
𝒟
2
𝑡
𝑟
⋯
superscript
subscript
𝒟
𝑁
𝑡
𝑟
{\mathcal{D}}^{tr}=\{{\mathcal{D}}_{1}^{tr},{\mathcal{D}}_{2}^{tr},\cdots,{%
\mathcal{D}}_{N}^{tr}\}
caligraphic_D start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT = { caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT , ⋯ , caligraphic_D start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT }
. The training data of 
k
𝑘
k
italic_k
-th task 
𝒟
k
t
⁢
r
superscript
subscript
𝒟
𝑘
𝑡
𝑟
{\mathcal{D}}_{k}^{tr}
caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT
 consists of a set of triplets 
{
(
𝒙
i
k
,
y
i
k
,
𝒯
k
)
i
=
1
n
k
}
superscript
subscript
superscript
subscript
𝒙
𝑖
𝑘
superscript
subscript
𝑦
𝑖
𝑘
subscript
𝒯
𝑘
𝑖
1
subscript
𝑛
𝑘
\{({\bm{x}}_{i}^{k},y_{i}^{k},{\mathcal{T}}_{k})_{i=1}^{n_{k}}\}
{ ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , caligraphic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT }
, where 
𝒙
i
k
superscript
subscript
𝒙
𝑖
𝑘
{\bm{x}}_{i}^{k}
bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT
 is the 
i
𝑖
i
italic_i
-th data example, 
y
i
k
superscript
subscript
𝑦
𝑖
𝑘
y_{i}^{k}
italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT
 is the data label associated with 
𝒙
i
k
superscript
subscript
𝒙
𝑖
𝑘
{\bm{x}}_{i}^{k}
bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT
, and 
𝒯
k
subscript
𝒯
𝑘
{\mathcal{T}}_{k}
caligraphic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT
 is the task identifier. The goal is to learn a neural network with parameters 
𝜽
𝜽
{\bm{\theta}}
bold_italic_θ
, i.e., 
f
𝜽
subscript
𝑓
𝜽
f_{{\bm{\theta}}}
italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT
, on 
𝒟
t
⁢
r
superscript
𝒟
𝑡
𝑟
{\mathcal{D}}^{tr}
caligraphic_D start_POSTSUPERSCRIPT italic_t italic_r end_POSTSUPERSCRIPT
 so that it performs well on the test set of all the learned tasks 
𝒟
t
⁢
e
=
{
𝒟
1
t
⁢
e
,
𝒟
2
t
⁢
e
,
⋯
,
𝒟
N
t
⁢
e
}
superscript
𝒟
𝑡
𝑒
superscript
subscript
𝒟
1
𝑡
𝑒
superscript
subscript
𝒟
2
𝑡
𝑒
⋯
superscript
subscript
𝒟
𝑁
𝑡
𝑒
{\mathcal{D}}^{te}=\{{\mathcal{D}}_{1}^{te},{\mathcal{D}}_{2}^{te},\cdots,{%
\mathcal{D}}_{N}^{te}\}
caligraphic_D start_POSTSUPERSCRIPT italic_t italic_e end_POSTSUPERSCRIPT = { caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_e end_POSTSUPERSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_e end_POSTSUPERSCRIPT , ⋯ , caligraphic_D start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_e end_POSTSUPERSCRIPT }
 without forgetting the knowledge of previous tasks.

Figure 1: 
Categorization of existing continual learning approach.

Existing methods on task-aware CL have explored five main branches: memory-based, architecture-based, regularization-based, subspace-based, and Bayesian-based methods. An overview of these branches is provided in Figure 
1
. For a more comprehensive description of the methods within each category, please refer to Appendix 
A.1
. Below, we provide a brief description of each class method.

Memory-based Method

Memory-based method keeps a 
memory buffer
 that stores the data examples from previous tasks and replays those examples during learning new tasks. It can be further categorized into: raw memory replay; memory sample selection; generative replay; and compressed memory replay. Next, we discuss each direction in detail.
(1) 
Raw Sample Replay:
 These methods randomly save a small amount of raw data from previous tasks and train the model together with the new task data. When the new task updates the model, the old task data is used as a constraint 
[
15
, 
16
]
 or directly mixed with the new data to form a batch 
[
17
]
 to update the model, thereby alleviating forgetting.
(2) 
Memory Sample Selection:
 Randomly selecting samples for replay ignores the amount of information in each sample, which can lead to suboptimal performance 
[
18
, 
19
]
. Therefore, 
heuristic selection
 selects samples to be stored according to certain rules. For example, select the representative sample closest to the cluster center 
[
20
]
, the samples with higher diversity 
[
21
, 
22
]
, or the difficult sample closer to the decision boundary 
[
23
, 
24
]
.
(3) 
Generative Replay:
 When privacy concerns restrict the storage of raw memory data, generative replay provides an alternative approach in CL to replay previous task data. The main concept behind generative replay is to train a generative model capable of capturing and remembering the data distribution from previous tasks. The representative works include GAN-based 
[
25
, 
26
]
, AutoEncoder-based 
[
27
]
, Diffusion-based 
[
28
]
, and Model-inversion 
[
29
]
.
(4) 
Compressed Memory Replay:
 In scenarios with strict storage constraints on edge devices, memory efficiency becomes a critical consideration. Existing works store feature representations 
[
30
, 
31
]
 or low-fidelity images 
[
32
, 
33
]
 instead of original images, or learning a set of condensed images 
[
34
, 
35
, 
36
]
 using dataset distillation 
[
37
]
.

Architecture-based Method

Architecture-based methods in CL 
[
38
, 
39
, 
40
]
 involve updating the network architecture during the learning process to retain previously acquired knowledge. These methods aim to adapt the model’s architecture to acquire new tasks while preserving the knowledge from previous tasks.
Based on whether the model parameters expand with the number of tasks, architecture-based methods can be categorized into two types: fixed-capacity and capacity-increasing methods.
(1) 
Fixed-Capacity
: In these methods, the amount of CL model’s parameters does not increase with the number of tasks, and each task selects a sub-network from the CL model to achieve knowledge transfer and reduce the forgetting caused by sub-network updates. Common subnetwork selection techniques include masking 
[
41
, 
42
, 
43
]
, and pruning 
[
44
, 
45
, 
46
]
.
(2) 
Capacity-Increasing
: As the number of tasks increases, fixed-capacity CL models may face limitations in accommodating new tasks. To overcome this challenge, dynamic capacity methods are proposed 
[
38
, 
40
, 
47
, 
48
]
. These methods ensure that old tasks are not forgotten and adapt to new tasks by introducing new task-specific parameters for each new task, while freezing parameters related to old tasks.

Regularization-based Method

These methods in CL involve the addition of regularization loss terms to the training objective to prevent forgetting previously learned knowledge 
[
49
, 
50
, 
51
]
.
It can be further divided into two subcategories: penalizing important parameter updates and knowledge distillation using a previous model as a teacher.
(1) 
Penalize Parameter Updates:
 These methods use the Fisher information matrix 
[
49
]
 or the cumulative update amount of parameters 
[
52
]
 as a measure of the importance of old task parameters. On the one hand, when new tasks update important parameters, a large penalty is imposed in order to keep the knowledge of old tasks from being forgotten. On the other hand, imposing a small penalty on unimportant parameter updates helps learn new task’s knowledge 
[
50
, 
53
, 
23
]
.
(2) 
Knowledge-Distillation-Based:
 Several methods in CL incorporate a knowledge distillation 
[
54
]
 loss between the network of the previous task (referred to as the teacher) and the network of the current task (referred to as the student) to mitigate forgetting 
[
55
, 
56
, 
57
]
. It should be mentioned that the ideal scenario would involve using raw data from old tasks to extract the knowledge of the teacher model and refine it into the student model. However, accessing raw data of old tasks is often not feasible due to data privacy concerns. Therefore, existing methods utilize proxy data, such as new task data 
[
55
]
 or large-scale unlabeled data 
[
58
]
, as a substitute for distillation.

Subspace-based Method

Subspace-based methods in CL aim to address the issue of interference between multiple tasks by conducting learning in separate and disjoint subspaces, thus reducing old task forgetting. Subspace-based methods can be categorized into two types based on how the subspaces are constructed:
(1) 
Orthogonal Gradient Subspace
: These methods require that the parameter update direction of the new task is orthogonal to the gradient subspace of the old tasks 
[
59
, 
60
, 
61
]
, ensuring minimal interference between tasks.
(2) 
Orthogonal Feature Subspace
: These require that the parameter update direction of the new task is orthogonal to the subspace spanned by the input(feature) of the old tasks 
[
62
, 
63
, 
64
, 
65
, 
66
]
.

We illustrate the working principle of the subspace-based methods (i.e., the orthogonal projection methods) in Fig. 
2
.
Specifically, we define the core subspace (CS) spanned by task 1 in the 
l
t
⁢
h
superscript
𝑙
𝑡
ℎ
l^{th}
italic_l start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT
 layer as 
S
l
superscript
𝑆
𝑙
\small S^{l}
italic_S start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, constructed from the gradients or features of task 1. The orthogonal space of the core subspace is denoted as the residual subspace (RS). When a new task 2 updates the network in the 
l
t
⁢
h
superscript
𝑙
𝑡
ℎ
l^{th}
italic_l start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT
 layer with parameters 
𝜽
l
superscript
𝜽
𝑙
\small{\bm{\theta}}^{l}
bold_italic_θ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, the original gradient direction 
𝐠
𝜽
l
(
2
)
superscript
subscript
𝐠
superscript
𝜽
𝑙
2
\small\mathbf{g}_{{\bm{\theta}}^{l}}^{(2)}
bold_g start_POSTSUBSCRIPT bold_italic_θ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT
 is decomposed into CS and RS components. Only the gradient component in the RS, given by 
𝐠
𝜽
l
(
2
)
−
Proj
S
l
⁢
(
𝐠
𝜽
l
(
2
)
)
superscript
subscript
𝐠
superscript
𝜽
𝑙
2
subscript
Proj
superscript
𝑆
𝑙
superscript
subscript
𝐠
superscript
𝜽
𝑙
2
\small\mathbf{g}_{{\bm{\theta}}^{l}}^{(2)}-\text{Proj}_{S^{l}}(\mathbf{g}_{{%
\bm{\theta}}^{l}}^{(2)})
bold_g start_POSTSUBSCRIPT bold_italic_θ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT - Proj start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_g start_POSTSUBSCRIPT bold_italic_θ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT )
, is used to update the parameter 
𝜽
l
superscript
𝜽
𝑙
\small{\bm{\theta}}^{l}
bold_italic_θ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
.

Figure 2: 
An illustration of the principle of orthogonal projection method.

Next, we demonstrate why subspace-based approaches (e.g., 
[
63
, 
65
, 
67
]
) can alleviate the forgetting problem. Let the network’s weight after training on task 1 be 
𝜽
1
l
superscript
subscript
𝜽
1
𝑙
\small{\bm{\theta}}_{1}^{l}
bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, and the weight update for task 2 be 
Δ
⁢
𝜽
2
l
Δ
superscript
subscript
𝜽
2
𝑙
\small\Delta{\bm{\theta}}_{2}^{l}
roman_Δ bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, resulting in the network’s weight after training on task 2 as 
𝜽
2
l
=
𝜽
1
l
+
Δ
⁢
𝜽
2
l
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝜽
1
𝑙
Δ
superscript
subscript
𝜽
2
𝑙
\small{\bm{\theta}}_{2}^{l}={\bm{\theta}}_{1}^{l}+\Delta{\bm{\theta}}_{2}^{l}
bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT + roman_Δ bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
.
Clearly, the input sample 
𝒙
1
,
i
l
superscript
subscript
𝒙
1
𝑖
𝑙
\small\boldsymbol{x}_{1,i}^{l}
bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
 from task 1 lies in the subspace 
S
l
superscript
𝑆
𝑙
\small S^{l}
italic_S start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
 spanned by task 1. Since the update for task 2 is performed along the subspace orthogonal to 
S
l
superscript
𝑆
𝑙
\small S^{l}
italic_S start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, we have 
Δ
⁢
𝜽
2
l
⁢
𝒙
1
,
i
l
=
0
Δ
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
0
\Delta{\bm{\theta}}_{2}^{l}\boldsymbol{x}_{1,i}^{l}=0
roman_Δ bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = 0
. In other words, if task 2 updates are restricted to the direction orthogonal to the subspace spanned by task 1, we ensure that 
𝜽
2
l
⁢
𝒙
1
,
i
l
=
𝜽
1
l
⁢
𝒙
1
,
i
l
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
superscript
subscript
𝜽
1
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
\small{\bm{\theta}}_{2}^{l}\boldsymbol{x}_{1,i}^{l}={\bm{\theta}}_{1}^{l}%
\boldsymbol{x}_{1,i}^{l}
bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT
, thereby preventing forgetting. The formal statement is as follows:

𝜽
2
l
⁢
𝒙
1
,
i
l
=
(
𝜽
1
l
+
Δ
⁢
𝜽
2
l
)
⁢
𝒙
1
,
i
l
=
𝜽
1
l
⁢
𝒙
1
,
i
l
+
Δ
⁢
𝜽
2
l
⁢
𝒙
1
,
i
l
=
𝜽
1
l
⁢
𝒙
1
,
i
l
.
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
superscript
subscript
𝜽
1
𝑙
Δ
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
superscript
subscript
𝜽
1
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
Δ
superscript
subscript
𝜽
2
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
superscript
subscript
𝜽
1
𝑙
superscript
subscript
𝒙
1
𝑖
𝑙
\small{\bm{\theta}}_{2}^{l}\boldsymbol{x}_{1,i}^{l}=\left({\bm{\theta}}_{1}^{l%
}+\Delta{\bm{\theta}}_{2}^{l}\right)\boldsymbol{x}_{1,i}^{l}={\bm{\theta}}_{1}%
^{l}\boldsymbol{x}_{1,i}^{l}+\Delta{\bm{\theta}}_{2}^{l}\boldsymbol{x}_{1,i}^{%
l}={\bm{\theta}}_{1}^{l}\boldsymbol{x}_{1,i}^{l}.
bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = ( bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT + roman_Δ bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT + roman_Δ bold_italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = bold_italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT .

(3)

In Appendix 
A.1
, we discuss in detail the respective advantages and disadvantages of gradient projection in feature space and gradient space, and how to choose them.

Bayesian Method

Bayesian approaches offer effective strategies to mitigate forgetting by incorporating uncertainty estimation and regularization techniques, thereby enhancing the adaptability of the learning process. Bayesian methods can be classified into three categories: (1) constrain the update of weight parameter distributions; (2) constrain the update in function space; and (3) dynamically grow the CL model architecture in an adaptive and Bayesian manner.
Specifically,
(1) 
Weight Space Regularization:
 These methods model the parameter update uncertainty and enforce the model parameter (weight space) distribution when learning the new task is close to that of all the previously learned tasks, including 
[
68
, 
69
, 
70
, 
71
, 
72
, 
73
]
.
(2) 
Function Space Regularization:
 Different from weight space regularization which constrains the weight update, the function space regularization regulates the CL function update in the function space.
They achieve this goal by enforcing the posterior distribution over the function space 
[
74
]
, constraining neural network predictions 
[
75
]
, modeling the cross-task covariances 
[
76
]
 or sequential function-space variational inference 
[
77
]
.
(3) 
Bayesian Architecture Expansion:
 Bayesian architecture growing methods employ a probabilistic and Bayesian approach to dynamically expand the CL model. This probabilistic framework facilitates the flexible and principled expansion of the model’s architecture, allowing it to accommodate increasing complexity and variability in the learning process, including 
[
78
, 
79
]
.

In Appendix 
A.1
, we discuss the differences and connections between weight space regularity and function space regularity in detail.

2.1.2 
Task-free CL

Task-free CL assumes that the learning system does not have access to any explicit task information. Unlike the task-aware CL setting, where a sequence of tasks is defined, task-free CL aims to perform adaptation without explicit task boundaries.
The system needs to adapt and generalize its knowledge over time, continually updating its model or representation to accommodate new information while retaining previously learned knowledge.
Existing approaches for task-free CL can be categorized into two classes: memory-based methods and network expansion-based methods.

Memory-based method

Memory-based methods 
[
21
, 
24
, 
80
, 
16
]
 involve storing a small subset of previous data and replaying them alongside new mini-batch data. MIR 
[
24
]
 selects and replays samples that are most prone to interference. This selective replay aims to prioritize samples that are most relevant for retaining previously learned knowledge. Building upon MIR, GEN-MIR 
[
24
]
 incorporates generative models to synthesize memory examples during replay. GSS 
[
21
]
 focuses on storing diverse examples. GMED 
[
81
]
, proposes a method for editing the memory examples to promote forgetting and discourage memorization. While GMED focuses on editing memory examples, Wang et al. 
[
82
]
 propose a Distributionally Robust Optimization framework that considers population- and distribution-level evolution to address memory overfitting.

Expansion-based method

In architecture expansion-based methods, several approaches have been proposed to address the forgetting issue and facilitate continual adaptation.
CN-DPM 
[
83
]
 introduces a method that expands the network structure based on the Dirichlet process mixture model.
This approach allows for the automatic expansion of the network to accommodate new data distributions or concepts while preserving previously learned knowledge. VariGrow 
[
84
]
 proposes a variational architecture growing method based on Bayesian novelty to identify novel information and dynamically expand the network architecture to accommodate new knowledge. ODDL 
[
85
]
 proposes a dynamical architecture expansion method based on estimating the discrepancy between the probabilistic representation of the memory buffer data and the accumulated knowledge.

2.2 
Online CL

2.2.1 
Approaches for Online CL

In online CL, the learner is only allowed to process the data for each task once 
[
5
]
. Existing works addressing forgetting in online CL are mainly based on 
rehearsal replay
 
[
24
, 
21
, 
86
, 
87
, 
88
]
.
MIR 
[
24
]
 suggests replaying the samples that exhibit the maximum increase in loss. OCS 
[
89
]
 proposes to select samples with high affinity for old tasks.
DVC 
[
90
]
 introduces a technique that involves selecting samples whose gradients are most interfered with new incoming samples to store in memory buffer.
ASER 
[
86
]
 introduces an adversarial Shapley value method to score memory data samples based on their contribution to forgetting
La-MAML 
[
91
]
 utilizes a meta-learning algorithm to tackle online CL by leveraging a small episodic memory.
POCL 
[
88
]
 reformulates replay-based online CL into a hierarchical gradient aggregation framework and enhances past task performance while maintaining current task performance using Pareto optimization.
In addition, some studies have proposed 
regularization-based strategies
 to prevent forgetting 
[
92
, 
93
]
.

2.2.2 
Imbalanced Class Issue in Online CL

The presence of imbalanced data streams in online CL has drawn significant attention, primarily due to its prevalence in real-world application scenarios 
[
94
, 
95
, 
96
, 
97
]
. Addressing class imbalance can be approached through two main strategies: (1) learning a model that effectively balances the learning of both old and new classes during training, or (2) employing post-processing techniques to calibrate the biases inherent in the model.

Balance Learning Between New and Old Classes

Balancing in the training phase involves heuristically selecting a balanced memory to tune the model 
[
94
, 
95
, 
98
, 
99
]
. Chrysakis et al. 
[
98
]
 propose class-balancing reservoir sampling (CBRS) to tackle this issue. PRS 
[
99
]
 suggests a partitioning reservoir sampling strategy to address this issue. Kim et al. 
[
100
]
 introduces a stochastic information-theoretic reservoir sampler to select memory points from the imbalanced data stream. E2E 
[
94
]
 proposes to alleviate the imbalance problem by adopting a balanced fine-tuning strategy at the end of each incremental stage. GDumb 
[
95
]
 found that the downsampling strategy can well solve the problem of imbalance between old and new classes.

Post-Processing Calibration Techniques

Post-processing calibration methods perform bias calibration on the classifier during inference phase 
[
96
, 
101
, 
102
]
. BiC 
[
96
]
 introduces a two-stage training where they perform the main training in the first stage, followed by a linear transformation to mitigate bias in the second stage. WA 
[
102
]
 reduces the imbalance between old and new classes by aligning the model logits outputs on the old and new classes.
OBC 
[
103
]
 provides both theoretical and empirical explanations of how replay can introduce a bias towards the most recently observed data stream. They modify the model’s output layer, aiming to mitigate the online bias.

2.3 
Semi-supervised, Few-shot and Unsupervised CL

2.3.1 
Semi-supervised CL

Semi-supervised CL is an extension of traditional CL that allows each task to incorporate unlabeled data as well.
Existing works mainly include generative replay 
[
104
, 
105
]
 and distillation 
[
58
, 
106
]
 to avoid forgetting. Specifically,
ORDisCo 
[
105
]
 maintains a relatively constant-sized network, and it simultaneously trains a classifier and a conditional GAN, and learns the classifier by replaying data sampled from the GAN in an online fashion. SDSL 
[
104
]
 is also based on the generation-replay framework. GD 
[
58
]
 and DistillMatch 
[
106
]
 are distillation-based approaches. DistillMatch performs knowledge distillation by assigning pseudo-labels and data augmentation to the unlabeled data. In particular, DietCL 
[
107
]
 explores semi-supervised CL scenarios with sparse labeled data and limited computational budget.

2.3.2 
Few-shot CL

Few-shot CL refers to the scenario where a model needs to learn new tasks with only a limited number of labeled examples per task while retaining knowledge from previously encountered tasks. The challenge lies in effectively leveraging the limited labeled data and previously learned knowledge to adapt to new tasks while avoiding forgetting.

Compared to traditional CL, few-shot CL faces the challenge of overfitting due to the limited number of examples available per task 
[
108
, 
109
]
. To tackle the forgetting problem in few-shot CL, existing approaches employ various techniques, including metric learning, meta-learning, and parameter regularization. Due to limited pages, we provide details of these methods in Appendix. 
A.2
.
Below, we briefly explain each method:
(1) 
Metric Learning-Based:
 These methods perform classification by class prototypes. To avoid forgetting, the prototype of the new class should be separable from the old class 
[
110
, 
111
, 
112
]
, and the prototype of the old class should not change drastically during the adjustment process of the new class 
[
112
, 
108
]
.
(2) 
Meta-Learning-Based:
 These methods simulate the inference phase during training so that CL models can quickly adapt to unseen new classes to solve few-shot CL. For example, LIMIT 
[
113
]
 and MetaFSCIL 
[
114
]
 split the base task into multiple ’fake’-incremental tasks, so that the model has the learning ability of few-shot CL tasks. By reducing the loss associated with the meta-objective, they minimize forgetting of the old tasks.
(3) 
Parameter Regularization-Based:
 These methods employ various strategies to address the forgetting problem by penalizing parameter updates that are important for old tasks 
[
115
, 
116
]
.

2.3.3 
Unsupervised CL

Unsupervised CL 
[
117
, 
118
]
 is a rapidly growing research area that emphasizes learning from unlabeled data alone. Unlike traditional supervised CL relying on labeled data, unsupervised CL explores techniques that enable learning and adaptation using only unlabeled data.

Existing unsupervised CL methods mainly rely on 
representation-based contrastive learning
 techniques 
[
117
, 
119
, 
118
, 
120
]
.
CURL 
[
117
]
 is the first offline continual
unsupervised representation learning framework with unknown task labels and boundaries. Co2l 
[
119
]
 finds that self-supervised loss is generally more robust to forgetting than cross-entropy loss in CL.
LUMP 
[
118
]
 observes that unsupervised CL models have a flatter loss landscape than supervised CL models, and additionally, it performs Mixup 
[
121
]
 between old task samples and new task samples to reduce forgetting.
Prob 
[
120
]
 revisits the phenomenon of representational forgetting in both supervised and unsupervised CL settings, and shows that using observed accuracy to measure forgetting is a misleading metric because a low model accuracy on old tasks does not necessarily indicate significant changes in the learned representations. This discrepancy suggests that accuracy alone is not a reliable indicator of the extent of forgetting in unsupervised CL.

Suitable metric for unsupervised CL
: Following 
[
118
]
 and centered kernel alignment (CKA) 
[
122
]
, we measure the similarity between the representations obtained using the parameters learned at the end of task 
t
𝑡
t
italic_t
 (i.e., 
𝜽
t
subscript
𝜽
𝑡
{\bm{\theta}}_{t}
bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
) and those obtained using the parameters learned at the end of the last task 
T
𝑇
T
italic_T
 (i.e., 
𝜽
T
subscript
𝜽
𝑇
{\bm{\theta}}_{T}
bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT
) to evaluate representation forgetting. Intuitively, a higher similarity indicates less forgetting of previous data distributions. Specifically, the similarity is calculated using test data from task 
t
𝑡
t
italic_t
, as formulated below:

sim
⁢
(
𝜽
t
,
𝜽
T
;
X
)
=
HSIC
⁢
(
A
,
B
)
HSIC
⁢
(
A
,
A
)
⁢
HSIC
⁢
(
B
,
B
)
,
sim
subscript
𝜽
𝑡
subscript
𝜽
𝑇
𝑋
HSIC
𝐴
𝐵
HSIC
𝐴
𝐴
HSIC
𝐵
𝐵
\centering\small\text{sim}\left({\bm{\theta}}_{t},{\bm{\theta}}_{T};X\right)=%
\frac{\text{HSIC}(A,B)}{\sqrt{\text{HSIC}(A,A)\text{HSIC}(B,B)}},\@add@centering
sim ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ; italic_X ) = divide start_ARG HSIC ( italic_A , italic_B ) end_ARG start_ARG square-root start_ARG HSIC ( italic_A , italic_A ) HSIC ( italic_B , italic_B ) end_ARG end_ARG ,

(4)

where HSIC denotes Hilbert-Schmidt Independence Criterion, as defined in 
[
122
]
. where 
(
A
)
i
,
j
=
a
⁢
(
𝒛
i
,
𝒛
j
)
subscript
𝐴
𝑖
𝑗
𝑎
subscript
𝒛
𝑖
subscript
𝒛
𝑗
(A)_{i,j}=a({\bm{z}}_{i},{\bm{z}}_{j})
( italic_A ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_a ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )
, 
a
(
,
)
a(,)
italic_a ( , )
 denotes a kernel function. 
(
A
)
i
,
j
subscript
𝐴
𝑖
𝑗
(A)_{i,j}
( italic_A ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT
 denotes the element in 
i
𝑖
i
italic_i
-th row and 
j
𝑗
j
italic_j
-th column of matrix 
A
𝐴
A
italic_A
. 
(
B
)
i
,
j
=
b
⁢
(
𝒖
i
,
𝒖
j
)
subscript
𝐵
𝑖
𝑗
𝑏
subscript
𝒖
𝑖
subscript
𝒖
𝑗
(B)_{i,j}=b({\bm{u}}_{i},{\bm{u}}_{j})
( italic_B ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_b ( bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )
, where 
b
(
,
)
b(,)
italic_b ( , )
 denotes a kernel function.

X
𝑋
X
italic_X
 represents the test data of task 
t
𝑡
t
italic_t
. 
𝒛
𝒛
{\bm{z}}
bold_italic_z
 and 
𝒖
𝒖
{\bm{u}}
bold_italic_u
 are the representations w.r.t 
X
𝑋
X
italic_X
 extracted by the CL model 
f
𝑓
f
italic_f
 with weights 
𝜽
t
subscript
𝜽
𝑡
{\bm{\theta}}_{t}
bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 and 
𝜽
T
subscript
𝜽
𝑇
{\bm{\theta}}_{T}
bold_italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT
, respectively.

CKA is more suitable for evaluating representation forgetting in unsupervised CL for the following reasons:
(1)
CKA can capture both linear and non-linear relationships between representations. Cosine similarity, on the other hand, is limited to linear relationships and measures only the angle between two vectors, ignoring more complex relationships.
(2) CKA is invariant to isotropic scaling and orthogonal transformations.
Cosine similarity only accounts for the direction of the vectors and is not invariant to shifts in data, which can affect the similarity measurement if data undergoes certain transformations.

2.4 
Theoretical Analysis

The 
theoretical
 analysis of CL is quite a few. Pentina et al. 
[
123
]
 provide a generalization bound in the PAC-Bayesian framework for CL. Karakida et al. 
[
124
]
 conduct a theoretical analysis of the generalization performance within a solvable case of CL. They utilized a statistical mechanical analysis of kernel ridge-less regression to provide insights into the generalization capabilities in CL.
Kim et al. 
[
125
]
 study class-incremental learning and provides a theoretical justification for decomposing the problem into task-id prediction and within-task prediction.
Evron et al. 
[
126
]
 theoretically study the CL on a sequence of separable linear classification tasks with binary classes. Peng et al. 
[
127
]
 propose Ideal Continual Learner (ICL), which unifies multiple existing well-established CL solutions, and gives the generalization bound of ICL. Zhao et al. 
[
128
]
 statistically analyze regularization-based CL on linear regression tasks, highlighting the impact of various regularization terms on model performance.

According to 
[
127
]
, which provides a generalization analysis for continual learner.
Each task optimizes the following learning objective:

𝒢
t
:=
arg
⁢
min
𝜽
∈
𝚯
⁡
ℒ
⁢
(
𝜽
,
𝒟
t
)
assign
subscript
𝒢
𝑡
subscript
arg
min
𝜽
𝚯
ℒ
𝜽
subscript
𝒟
𝑡
{\mathcal{G}}_{t}:=\operatorname*{arg\,min}_{{\bm{\theta}}\in{\bm{\Theta}}}{%
\mathcal{L}}({\bm{\theta}},{\mathcal{D}}_{t})
caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT := start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_θ ∈ bold_Θ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ , caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
. Let 
c
t
∗
=
min
𝜽
⁡
𝔼
(
𝒙
,
y
)
∼
𝒟
t
⁢
ℒ
⁢
(
𝜽
,
𝒙
,
y
)
subscript
superscript
𝑐
𝑡
subscript
𝜽
subscript
𝔼
similar-to
𝒙
𝑦
subscript
𝒟
𝑡
ℒ
𝜽
𝒙
𝑦
c^{*}_{t}=\min_{{\bm{\theta}}}{\mathbb{E}}_{({\bm{x}},y)\sim{\mathcal{D}}_{t}}%
{\mathcal{L}}({\bm{\theta}},{\bm{x}},y)
italic_c start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( bold_italic_x , italic_y ) ∼ caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ , bold_italic_x , italic_y )
; 
d
𝑑
d
italic_d
 denotes the input dimension, i.e., 
𝒙
∈
ℝ
d
𝒙
superscript
ℝ
𝑑
{\bm{x}}\in{\mathbb{R}}^{d}
bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT
; 
‖
𝜽
‖
2
≤
R
subscript
norm
𝜽
2
𝑅
||{\bm{\theta}}||_{2}\leq R
| | bold_italic_θ | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_R
; 
I
t
subscript
𝐼
𝑡
I_{t}
italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 denotes the number of training examples for task 
t
𝑡
t
italic_t
.

Assumption 2.1

Assume all tasks share a global minimizer, i.e., 
∩
t
=
1
t
=
N
𝒢
t
≠
∅
superscript
subscript
𝑡
1
𝑡
𝑁
subscript
𝒢
𝑡
\cap_{t=1}^{t=N}{\mathcal{G}}_{t}\neq\emptyset
∩ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t = italic_N end_POSTSUPERSCRIPT caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≠ ∅
.

Assumption 2.2

Assume the CL loss function is 
K
𝐾
K
italic_K
-Lipschitz, i.e., 
|
ℒ
⁢
(
𝛉
,
𝐱
,
y
)
−
ℒ
⁢
(
𝛉
′
,
𝐱
,
y
)
|
≤
K
⁢
‖
𝛉
−
𝛉
′
‖
2
ℒ
𝛉
𝐱
𝑦
ℒ
superscript
𝛉
′
𝐱
𝑦
𝐾
subscript
norm
𝛉
superscript
𝛉
′
2
\small|{\mathcal{L}}({\bm{\theta}},{\bm{x}},y)-{\mathcal{L}}({\bm{\theta}}^{%
\prime},{\bm{x}},y)|\leq K||{\bm{\theta}}-{\bm{\theta}}^{\prime}||_{2}
| caligraphic_L ( bold_italic_θ , bold_italic_x , italic_y ) - caligraphic_L ( bold_italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , bold_italic_x , italic_y ) | ≤ italic_K | | bold_italic_θ - bold_italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT
.

The above two assumptions ensure the following uniform convergence 
[
129
]
 for 
δ
∈
(
0
,
1
)
𝛿
0
1
\delta\in(0,1)
italic_δ ∈ ( 0 , 1 )
:

|
∑
i
=
1
i
=
I
t
ℒ
⁢
(
𝜽
,
𝒙
i
,
y
i
)
−
𝔼
(
𝒙
,
y
)
∼
𝒟
t
⁢
ℒ
⁢
(
𝜽
,
𝒙
,
y
)
|
≤
ζ
⁢
(
I
t
,
δ
)
superscript
subscript
𝑖
1
𝑖
subscript
𝐼
𝑡
ℒ
𝜽
subscript
𝒙
𝑖
subscript
𝑦
𝑖
subscript
𝔼
similar-to
𝒙
𝑦
subscript
𝒟
𝑡
ℒ
𝜽
𝒙
𝑦
𝜁
subscript
𝐼
𝑡
𝛿
\displaystyle\small|\sum_{i=1}^{i=I_{t}}{\mathcal{L}}({\bm{\theta}},{\bm{x}}_{%
i},y_{i})-{\mathbb{E}}_{({\bm{x}},y)\sim{\mathcal{D}}_{t}}{\mathcal{L}}({\bm{%
\theta}},{\bm{x}},y)|\leq\zeta(I_{t},\delta)
| ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i = italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT caligraphic_L ( bold_italic_θ , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - blackboard_E start_POSTSUBSCRIPT ( bold_italic_x , italic_y ) ∼ caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ , bold_italic_x , italic_y ) | ≤ italic_ζ ( italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_δ )

(5)

where 
ζ
⁢
(
I
t
,
δ
)
=
𝒪
⁢
(
K
⁢
R
⁢
d
⁢
log
⁡
(
I
t
)
⁢
log
⁡
(
d
/
δ
)
I
t
)
𝜁
subscript
𝐼
𝑡
𝛿
𝒪
𝐾
𝑅
𝑑
subscript
𝐼
𝑡
𝑑
𝛿
subscript
𝐼
𝑡
\small\zeta(I_{t},\delta)={\mathcal{O}}(\frac{KR\sqrt{d\log(I_{t})\log(d/%
\delta)}}{\sqrt{I_{t}}})
italic_ζ ( italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_δ ) = caligraphic_O ( divide start_ARG italic_K italic_R square-root start_ARG italic_d roman_log ( italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log ( italic_d / italic_δ ) end_ARG end_ARG start_ARG square-root start_ARG italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG )

Theorem 2.3
 (Generalization Error)

Assume assumption 
2.1
 and 
2.2
 holds. With probability at least of 
1
−
δ
1
𝛿
1-\delta
1 - italic_δ
, there is the following generalization bound:

c
t
∗
≤
𝔼
(
𝒙
,
y
)
∼
𝒟
t
⁢
ℒ
⁢
(
𝜽
∗
,
𝒙
,
y
)
≤
c
t
∗
+
ζ
⁢
(
I
t
,
δ
)
subscript
superscript
𝑐
𝑡
subscript
𝔼
similar-to
𝒙
𝑦
subscript
𝒟
𝑡
ℒ
superscript
𝜽
𝒙
𝑦
subscript
superscript
𝑐
𝑡
𝜁
subscript
𝐼
𝑡
𝛿
\displaystyle\small c^{*}_{t}\leq{\mathbb{E}}_{({\bm{x}},y)\sim{\mathcal{D}}_{%
t}}{\mathcal{L}}({\bm{\theta}}^{*},{\bm{x}},y)\leq c^{*}_{t}+\zeta(I_{t},\delta)
italic_c start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≤ blackboard_E start_POSTSUBSCRIPT ( bold_italic_x , italic_y ) ∼ caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , bold_italic_x , italic_y ) ≤ italic_c start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ζ ( italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_δ )

(6)

where 
𝛉
∗
superscript
𝛉
{\bm{\theta}}^{*}
bold_italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT
 denotes the optimal global CL model parameters after completing the last task.

4 
Forgetting in Domain Adaptation

The objective of domain adaptation is to transfer knowledge from a source domain to a target domain. A domain represents the joint distribution of the input space 
𝒳
𝒳
{\mathcal{X}}
caligraphic_X
 and the output space 
𝒴
𝒴
{\mathcal{Y}}
caligraphic_Y
. Specifically, the source domain is defined as 
𝒫
S
⁢
(
𝒙
,
y
)
superscript
𝒫
𝑆
𝒙
𝑦
{\mathcal{P}}^{S}({\bm{x}},y)
caligraphic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ( bold_italic_x , italic_y )
, where 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 belongs to the input space 
𝒳
S
superscript
𝒳
𝑆
{\mathcal{X}}^{S}
caligraphic_X start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT
 and 
y
𝑦
y
italic_y
 belongs to the output space 
𝒴
S
superscript
𝒴
𝑆
{\mathcal{Y}}^{S}
caligraphic_Y start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT
. Similarly, the target domain is defined as 
𝒫
T
⁢
(
𝒙
,
y
)
superscript
𝒫
𝑇
𝒙
𝑦
{\mathcal{P}}^{T}({\bm{x}},y)
caligraphic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_italic_x , italic_y )
, where 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 belongs to the input space 
𝒳
T
superscript
𝒳
𝑇
{\mathcal{X}}^{T}
caligraphic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT
 and 
y
𝑦
y
italic_y
 belongs to the output space 
𝒴
T
superscript
𝒴
𝑇
{\mathcal{Y}}^{T}
caligraphic_Y start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT
.

In continual domain adaptation (CDA) 
[
164
]
, the focus is primarily on the covariate shift setting. Covariate shift refers to a situation where the distribution of input data, 
𝒳
𝒳
{\mathcal{X}}
caligraphic_X
, differs between the source and target domains, while the conditional distribution of the output, 
𝒴
𝒴
{\mathcal{Y}}
caligraphic_Y
, remains the same. This setting assumes that the relationship between inputs and outputs remains consistent across domains, but the distributions of the input data vary. This is formally defined as the following:

𝒫
S
⁢
(
X
=
𝒙
)
≠
𝒫
T
⁢
(
X
=
𝒙
)
,
𝒫
S
⁢
(
y
|
X
=
𝒙
)
=
𝒫
T
⁢
(
y
|
X
=
𝒙
)
.
formulae-sequence
superscript
𝒫
𝑆
𝑋
𝒙
superscript
𝒫
𝑇
𝑋
𝒙
superscript
𝒫
𝑆
conditional
𝑦
𝑋
𝒙
superscript
𝒫
𝑇
conditional
𝑦
𝑋
𝒙
\small{\mathcal{P}}^{S}(X={\bm{x}})\neq{\mathcal{P}}^{T}(X={\bm{x}}),{\mathcal%
{P}}^{S}(y|X={\bm{x}})={\mathcal{P}}^{T}(y|X={\bm{x}}).\!\!
caligraphic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ( italic_X = bold_italic_x ) ≠ caligraphic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( italic_X = bold_italic_x ) , caligraphic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ( italic_y | italic_X = bold_italic_x ) = caligraphic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( italic_y | italic_X = bold_italic_x ) .

(7)

CDA and traditional CL have distinct characteristics and goals.
On the one hand, CDA differs from traditional CL in terms of the availability of source domain data for transferring knowledge across the target domain sequence. In CDA, the source domain data is accessible, and the objective is to adapt the model from the source domain to the target domain, leveraging the available source domain data. However, the target domain may only provide unlabeled data, requiring the model to adapt to the new domain without explicit supervision.
On the other hand, traditional CL aims to learn and adapt the model to a sequence of tasks without accessing previous task-specific labeled data. Labeled data is typically provided for each task in traditional CL.

Problem Setup:

Suppose we have a pre-trained model 
f
𝜽
subscript
𝑓
𝜽
f_{{\bm{\theta}}}
italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT
 that has been trained on a set of source domain data 
𝒫
S
⁢
(
𝒙
,
y
)
superscript
𝒫
𝑆
𝒙
𝑦
{\mathcal{P}}^{S}({\bm{x}},y)
caligraphic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ( bold_italic_x , italic_y )
, where 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 belongs to the source domain input space 
𝒳
S
superscript
𝒳
𝑆
{\mathcal{X}}^{S}
caligraphic_X start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT
 and 
y
𝑦
y
italic_y
 belongs to the source domain label space 
𝒴
S
superscript
𝒴
𝑆
{\mathcal{Y}}^{S}
caligraphic_Y start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT
. Additionally, we have a sequence of evolving target distributions 
𝒫
t
T
⁢
(
𝒙
,
y
)
superscript
subscript
𝒫
𝑡
𝑇
𝒙
𝑦
{\mathcal{P}}_{t}^{T}({\bm{x}},y)
caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_italic_x , italic_y )
, where 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 belongs to the input space 
𝒳
t
T
subscript
superscript
𝒳
𝑇
𝑡
{\mathcal{X}}^{T}_{t}
caligraphic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 of the target domain 
t
𝑡
t
italic_t
. 
y
𝑦
y
italic_y
 belongs to the label space 
𝒴
t
T
subscript
superscript
𝒴
𝑇
𝑡
{\mathcal{Y}}^{T}_{t}
caligraphic_Y start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 of the target domain 
t
𝑡
t
italic_t
. 
t
𝑡
t
italic_t
 represents the domain index ranging from 1 to 
N
𝑁
N
italic_N
.
The objective of CDA, as proposed in 
[
164
]
, is to train 
f
𝜽
subscript
𝑓
𝜽
f_{{\bm{\theta}}}
italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT
 in such a way that it performs well on all the domains 
𝒫
t
T
⁢
(
𝒙
,
y
)
superscript
subscript
𝒫
𝑡
𝑇
𝒙
𝑦
{\mathcal{P}}_{t}^{T}({\bm{x}},y)
caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_italic_x , italic_y )
 in the evolving target domain sequence, defined as follows:

min
𝜽
⁡
𝔼
t
∈
[
1
,
…
,
N
]
⁢
𝔼
𝒙
T
∼
P
t
T
,
𝒙
S
∼
P
S
⁢
ℒ
⁢
(
f
𝜽
⁢
(
𝒙
T
)
,
f
𝜽
⁢
(
𝒙
S
)
)
.
subscript
𝜽
subscript
𝔼
𝑡
1
…
𝑁
subscript
𝔼
formulae-sequence
similar-to
superscript
𝒙
𝑇
subscript
superscript
𝑃
𝑇
𝑡
similar-to
superscript
𝒙
𝑆
superscript
𝑃
𝑆
ℒ
subscript
𝑓
𝜽
superscript
𝒙
𝑇
subscript
𝑓
𝜽
superscript
𝒙
𝑆
\small\min_{{\bm{\theta}}}\mathbb{E}_{t\in[1,\ldots,N]}\mathbb{E}_{{\bm{x}}^{T%
}\sim P^{T}_{t},{\bm{x}}^{S}\sim P^{S}}\mathcal{L}\left(f_{{\bm{\theta}}}({\bm%
{x}}^{T}),f_{{\bm{\theta}}}({\bm{x}}^{S})\right).
roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t ∈ [ 1 , … , italic_N ] end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ∼ italic_P start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ∼ italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) , italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ) ) .

(8)

Where 
𝔼
𝔼
\mathbb{E}
blackboard_E
 denotes expectation, 
ℒ
ℒ
\mathcal{L}
caligraphic_L
 denotes the KL divergence if 
f
𝜽
⁢
(
𝒙
T
)
,
f
𝜽
⁢
(
𝒙
S
)
subscript
𝑓
𝜽
superscript
𝒙
𝑇
subscript
𝑓
𝜽
superscript
𝒙
𝑆
f_{{\bm{\theta}}}({\bm{x}}^{T}),f_{{\bm{\theta}}}({\bm{x}}^{S})
italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) , italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT )
 represent the model output logits or 
l
1
,
l
2
subscript
𝑙
1
subscript
𝑙
2
l_{1},l_{2}
italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT
 distance function if they denote the feature representations.

When learning new target domains, their data distributions differ from that of the source domain. As a result, adapting the model to new domains can lead to forgetting the knowledge acquired from previous domains. Various methods have been developed to tackle the forgetting issue.

When the source domain data is available, most of the works avoid forgetting by replaying the source domain data 
[
165
, 
166
, 
167
]
, and a few works are based on regularization 
[
168
]
, or meta-learning 
[
169
]
.
First, CUA 
[
165
]
, addresses forgetting by randomly selecting samples from previous domains and stores them in a memory buffer. UCL-GV 
[
170
]
 utilizes a First-In, First-Out (FIFO) buffer to replay episodic memory.
AuCID 
[
166
]
 tackles CDA by consolidating the learned internal distribution. It achieves this by storing a fixed number of confident samples for each class per domain, which are later replayed during the adaptation process.
Then, GRCL 
[
168
]
 utilizes the gradient direction of samples from the previous domain as a 
regularization
 term. This constraint ensures that the model can be updated with new target domain data without negatively affecting the performance of the previous domains.
Finally, Meta-DR 
[
169
]
 proposes a 
meta-learning
 and domain randomization approach to mitigate forgetting and retain knowledge from previous domains during CDA.

Recently, few works have focused on 
source-free
 approaches 
[
171
]
 that protect the source domain data privacy, which is often inaccessible in many scenarios 
[
172
, 
173
]
. CoSDA 
[
172
]
 introduces a knowledge distillation method that employs a dual-speed teacher-student structure. The slow-updating teacher preserves the long-term knowledge of previous domains, while the fast-updating student quickly adapts to the target domain. C-SUDA 
[
173
]
 synthesizes source-style images to prevent forgetting.

5 
Forgetting in Test Time Adaptation

Test time adaptation (TTA) refers to the process of adapting a pre-trained model to unlabeled test data during inference or testing 
[
174
, 
175
, 
176
, 
177
, 
178
]
. Unlike domain adaptation, TTA occurs during the deployment phase rather than during the training phase.
In traditional machine learning scenarios, during testing, it is typically assumed that the test data 
𝒟
t
⁢
e
⁢
s
⁢
t
subscript
𝒟
𝑡
𝑒
𝑠
𝑡
{\mathcal{D}}_{test}
caligraphic_D start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT
 follows the same distribution as the training data. However, in real-world applications, it is common for the test data distribution to deviate from the training data distribution.
To address the distribution shift, TTA adapts the pre-trained model on the unlabeled testing data 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 using an unsupervised adaptation loss. This adaptation aims to minimize the loss function 
ℒ
⁢
(
𝒙
,
𝜽
)
ℒ
𝒙
𝜽
{\mathcal{L}}({\bm{x}},{\bm{\theta}})
caligraphic_L ( bold_italic_x , bold_italic_θ )
 with respect to the parameters 
𝜽
𝜽
{\bm{\theta}}
bold_italic_θ
. It is important to note that 
𝒙
𝒙
{\bm{x}}
bold_italic_x
 is sampled from the testing dataset, 
𝒟
t
⁢
e
⁢
s
⁢
t
subscript
𝒟
𝑡
𝑒
𝑠
𝑡
{\mathcal{D}}_{test}
caligraphic_D start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT
.
Subsequently, the adapted model utilizes the updated parameters to make predictions for the test input 
𝒙
𝒙
{\bm{x}}
bold_italic_x
. This allows the model to account for the distribution shift between training and testing, and hopefully improve its performance on test data.

Existing Works:

Tent 
[
174
]
 minimizes the entropy of model predictions on test data, thereby improving the model’s ability to generalize to unseen examples.
MECTA 
[
179
]
 proposes techniques to adapt the model during testing while optimizing memory usage, ensuring efficient and effective adaptation to the test data distribution.
MEMO 
[
180
]
 applies various data augmentations to a test data point. Subsequently, all model parameters are adapted by minimizing the entropy of the model’s output distribution across the augmented samples.

When a pre-trained model is adapted to new unlabeled test data, the model shifts to the new data, potentially causing it to forget crucial information previously learned from the in-distribution (ID) data. This phenomenon can result in a substantial loss of knowledge and adversely affect the model’s overall performance 
[
175
, 
181
]
. To address this issue, existing approaches primarily adopt two strategies.

Firstly, one approach is to replay a small portion of ID data during the
adaptation process to alleviate the forgetting issue. Without loss of generality, any data selection methods mentioned in the section (memory-based methods for continual learning) can be applied. For example, one can select the sample set closest to the class center, the sample set closest to the decision boundary, or the sample set with greater diversity. RMT 
[
182
]
 randomly samples 1% of the ID data. AUTO 
[
183
]
 stores one sample per class, which is initialized from randomly selected training samples. During training, samples from the same class are replaced according to a predefined rule.

Secondly, one can employ a two-step process to prevent the forgetting issue. Initially, the model trained on the ID data is frozen. Subsequently, new learnable parameters are introduced to adapt the model to test data 
[
184
, 
185
]
. For instance, VDP 
[
184
]
 prevents forgetting by freezing the model trained on the ID data and instead learns a set of visual prompts tailored to the test data. These prompts help the model adapt effectively to out-of-distribution (OOD) data. Similarly, EcoTTA 
[
185
]
 freezes the pre-trained network on ID data and introduces a lightweight meta-network to facilitate adaptation to OOD data while retaining the valuable ID data knowledge.

Thirdly, one can constrain the updates of important parameters to prevent forgetting in TTA. Tent 
[
174
]
 specifically focuses on preserving the previous knowledge by updating only the BatchNorm layer. CoTTA 
[
181
]
 randomly restores the weights of certain neurons to the weights that were originally trained on ID data. This restoration mechanism helps in retaining the knowledge acquired from the ID data.
Other approaches employ techniques similar to regularization-based approaches in traditional CL. These methods penalize the updating of parameters that are deemed important to ID data during TTA 
[
186
, 
175
, 
187
]
.
For instance, EATA 
[
175
]
 calculates the importance using the Fisher information matrix to penalize the parameters updates.

7 
Forgetting in Reinforcement Learning

While most existing CL methods primarily tackle the issue of forgetting in image classification, it is important to note that forgetting also widely occurs in reinforcement learning (RL), known as continual RL. Addressing forgetting in RL is vital for the advancement of intelligent agents that can continuously adapt to new tasks and environments 
[
200
]
.

Standard RL formulation could be defined as the following.
We denote 
𝒮
𝒮
{\mathcal{S}}
caligraphic_S
 as the state space, 
𝒜
𝒜
{\mathcal{A}}
caligraphic_A
 as the action space, and a reward function is 
r
⁢
(
s
t
,
a
t
)
:
𝒮
×
𝒜
→
R
:
𝑟
subscript
𝑠
𝑡
subscript
𝑎
𝑡
→
𝒮
𝒜
𝑅
r(s_{t},a_{t}):{\mathcal{S}}\times{\mathcal{A}}\rightarrow R
italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) : caligraphic_S × caligraphic_A → italic_R
, where 
r
⁢
(
s
t
,
a
t
)
𝑟
subscript
𝑠
𝑡
subscript
𝑎
𝑡
r(s_{t},a_{t})
italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 denotes the immediate reward received after taking action 
a
t
subscript
𝑎
𝑡
a_{t}
italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 in state 
s
t
subscript
𝑠
𝑡
s_{t}
italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
. At each time step 
t
𝑡
t
italic_t
, the agent sample action from a policy function which output the optimal action or the distributions over the action space. The deterministic policy takes the current state 
s
t
subscript
𝑠
𝑡
s_{t}
italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 as input, and outputs the optimal action 
a
t
subscript
𝑎
𝑡
a_{t}
italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 that should be performed at the current state 
s
t
subscript
𝑠
𝑡
s_{t}
italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 according to 
a
t
=
μ
⁢
(
s
t
)
subscript
𝑎
𝑡
𝜇
subscript
𝑠
𝑡
a_{t}=\mu(s_{t})
italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_μ ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
, where 
μ
𝜇
\mu
italic_μ
 denotes the deterministic policy network. A stochastic policy takes the state 
s
t
subscript
𝑠
𝑡
s_{t}
italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 as input, outputs the optimal action distribution according to 
a
t
∼
π
⁢
(
a
t
|
s
t
)
similar-to
subscript
𝑎
𝑡
𝜋
conditional
subscript
𝑎
𝑡
subscript
𝑠
𝑡
a_{t}\sim\pi(a_{t}|s_{t})
italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ italic_π ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
, where 
π
𝜋
\pi
italic_π
 denotes the stochastic policy network. Then, the state transition function takes the state 
s
t
subscript
𝑠
𝑡
s_{t}
italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 and action 
a
t
subscript
𝑎
𝑡
a_{t}
italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 as input and outputs the next state 
s
t
+
1
subscript
𝑠
𝑡
1
s_{t+1}
italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT
 either deterministically 
s
t
+
1
=
f
⁢
(
s
t
,
a
t
)
subscript
𝑠
𝑡
1
𝑓
subscript
𝑠
𝑡
subscript
𝑎
𝑡
s_{t+1}=f(s_{t},a_{t})
italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_f ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
 or stochastically 
s
t
+
1
∼
p
⁢
(
s
t
+
1
|
s
t
,
a
t
)
similar-to
subscript
𝑠
𝑡
1
𝑝
conditional
subscript
𝑠
𝑡
1
subscript
𝑠
𝑡
subscript
𝑎
𝑡
s_{t+1}\sim p(s_{t+1}|s_{t},a_{t})
italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ∼ italic_p ( italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
, where 
f
𝑓
f
italic_f
 denotes the deterministic state transition function and 
p
𝑝
p
italic_p
 denotes the stochastic state transition function. The goal of RL is to accumulate as much reward as possible. Following 
[
200
]
, the general continual RL can be defined as:

Definition 7.1

(General Continual RL): Given a state space 
𝒮
𝒮
{\mathcal{S}}
caligraphic_S
, action space 
𝒜
𝒜
{\mathcal{A}}
caligraphic_A
 and observation space 
𝒪
𝒪
{\mathcal{O}}
caligraphic_O
. A reward function is 
r
:
𝒮
×
𝒜
→
R
:
𝑟
→
𝒮
𝒜
𝑅
r:{\mathcal{S}}\times{\mathcal{A}}\rightarrow R
italic_r : caligraphic_S × caligraphic_A → italic_R
; A state transition function is 
p
:
𝒮
×
𝒜
→
𝒮
:
𝑝
→
𝒮
𝒜
𝒮
p:{\mathcal{S}}\times{\mathcal{A}}\rightarrow{\mathcal{S}}
italic_p : caligraphic_S × caligraphic_A → caligraphic_S
; An observation function is 
x
:
𝒮
→
𝒪
:
𝑥
→
𝒮
𝒪
x:{\mathcal{S}}\rightarrow{\mathcal{O}}
italic_x : caligraphic_S → caligraphic_O
. The general continual RL can be formulated as

ℳ
⁢
=
d
⁢
e
⁢
f
⁢
⟨
𝒮
⁢
(
t
)
,
𝒜
⁢
(
t
)
,
r
⁢
(
t
)
,
p
⁢
(
t
)
,
x
⁢
(
t
)
,
𝒪
⁢
(
t
)
⟩
.
ℳ
𝑑
𝑒
𝑓
𝒮
𝑡
𝒜
𝑡
𝑟
𝑡
𝑝
𝑡
𝑥
𝑡
𝒪
𝑡
\small{\mathcal{M}}\overset{def}{=}\langle{\mathcal{S}}(t),{\mathcal{A}}(t),r(%
t),p(t),x(t),{\mathcal{O}}(t)\rangle.
caligraphic_M start_OVERACCENT italic_d italic_e italic_f end_OVERACCENT start_ARG = end_ARG ⟨ caligraphic_S ( italic_t ) , caligraphic_A ( italic_t ) , italic_r ( italic_t ) , italic_p ( italic_t ) , italic_x ( italic_t ) , caligraphic_O ( italic_t ) ⟩ .

(9)

Definition 
7.1
 highlights that in continual RL, various components such as the state, action, reward, observation, and more, undergo changes over time. This emphasizes the dynamic nature of the RL process in continual settings.

Continual RL approach
. The existing continual RL methods can be categorized into four main groups: (1) 
regularization-based methods.
 These approaches employ techniques such as knowledge distillation to alleviate forgetting 
[
201
]
. Distillation can enhance experiences for training the policy or value function by offering an auxiliary target for the network to emulate. It is a widely-used technique for applying conservative updates, ensuring the agent’s learning remains stable and retains essential knowledge from previous tasks. (2) 
rehearsal-based methods.
 These methods utilize rehearsal or experience replay to mitigate forgetting 
[
202
]
. Specifically, a memory buffer is employed to generate realistic samples from previous experiences. Replay techniques help reduce short-term biases in the objective function by leveraging past experiences as approximations for future situations. Therefore, replay has emerged as a highly effective approach for managing continual RL. (3) 
architecture-based methods.
 These approaches focus on learning a shared structure, such as network modularity or composition, to facilitate continual learning 
[
203
]
. CL agents need to solve problems by finding useful patterns that help them in the future. They should reuse parts of previous solutions by forming abstract concepts or skills. Humans naturally break complex tasks into smaller ones and use knowledge from different timescales to plan and learn. Equipping continual RL agents with the ability to compose relevant modules from previous experiences will help them retain and transfer knowledge. (4) 
meta-learning-based methods
 
[
204
]
. Meta-learning is an effective method that boosts the learning efficiency of CL agents. By utilizing past successes and failures, the agent learns to refine its optimization processes during continual RL. If these refinements generalize effectively to future tasks, meta-learning creates an inductive bias that enhances the agent’s sample efficiency and adaptability in acquiring new behaviors. This capability is crucial for achieving efficient and adaptive continual RL.

Appendix B 
Forgetting in Meta-Learning

Meta-learning, also known as learning to learn, focuses on developing algorithms and models that can learn from previous learning experiences to improve their ability to learn new tasks or adapt to new domains more efficiently and effectively.
In meta-learning, the goal is to enable a learning system, often referred to as the meta-learner or the meta-model, to acquire general knowledge or "meta-knowledge" from a set of related learning tasks or domains. This meta-knowledge is then leveraged to facilitate faster learning, better generalization, and improved adaptation to new, unseen tasks or domains.

Formally, let’s consider a distribution of tasks, denoted as 
P
⁢
(
𝒯
)
𝑃
𝒯
P({\mathcal{T}})
italic_P ( caligraphic_T )
. For a specific task 
𝒯
t
subscript
𝒯
𝑡
{\mathcal{T}}_{t}
caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
, which consists of a training dataset 
𝒟
train
subscript
𝒟
train
{\mathcal{D}}_{\text{train}}
caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT
 and a validation dataset 
𝒟
val
subscript
𝒟
val
{\mathcal{D}}_{\text{val}}
caligraphic_D start_POSTSUBSCRIPT val end_POSTSUBSCRIPT
, sampled from the task distribution 
P
⁢
(
𝒯
)
𝑃
𝒯
P({\mathcal{T}})
italic_P ( caligraphic_T )
.
The loss function for task 
𝒯
t
subscript
𝒯
𝑡
{\mathcal{T}}_{t}
caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
 with meta-parameters 
𝜽
𝜽
{\bm{\theta}}
bold_italic_θ
 is defined as:

ℒ
⁢
(
𝒯
t
)
=
log
⁡
P
⁢
(
𝒟
val
|
𝒟
train
;
𝜽
)
.
ℒ
subscript
𝒯
𝑡
𝑃
conditional
subscript
𝒟
val
subscript
𝒟
train
𝜽
\small{\mathcal{L}}({\mathcal{T}}_{t})=\log P({\mathcal{D}}_{\text{val}}|{%
\mathcal{D}}_{\text{train}};{\bm{\theta}}).
caligraphic_L ( caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = roman_log italic_P ( caligraphic_D start_POSTSUBSCRIPT val end_POSTSUBSCRIPT | caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT ; bold_italic_θ ) .

(13)

The objective of meta-learning is to optimize the meta loss function, given by:

min
𝜽
⁡
𝔼
𝒯
t
∼
P
⁢
(
𝒯
)
⁢
ℒ
⁢
(
𝒯
t
)
.
subscript
𝜽
subscript
𝔼
similar-to
subscript
𝒯
𝑡
𝑃
𝒯
ℒ
subscript
𝒯
𝑡
\small\min_{{\bm{\theta}}}\mathbb{E}_{{\mathcal{T}}_{t}\sim P({\mathcal{T}})}{%
\mathcal{L}}({\mathcal{T}}_{t}).
roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ italic_P ( caligraphic_T ) end_POSTSUBSCRIPT caligraphic_L ( caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .

(14)

In other words, the aim is to find the optimal meta-parameters 
𝜽
𝜽
{\bm{\theta}}
bold_italic_θ
 that minimize the expected loss across tasks, where tasks are sampled from the task distribution 
P
⁢
(
𝒯
)
𝑃
𝒯
P({\mathcal{T}})
italic_P ( caligraphic_T )
.

However, forgetting can still occur in the context of meta-learning, and it can be classified into two distinct research directions.
The first research direction focuses on Incremental Few-Shot Learning (IFSL), where the objective is to meta-learn new classes in addition to the pre-trained base classes. In this scenario, forgetting arises from the loss of information related to the pre-trained base classes. The challenge lies in retaining the knowledge of both the base classes and the newly introduced classes during the learning process.
The second research direction deals with Continual Meta-Learning, where the agent encounters non-stationary task distributions over time while learning new tasks. Unlike IFSL, the goal here is not to remember the specific base classes. Instead, the objective is to retain the meta-knowledge acquired from previous task distributions. We will present the details of each direction in the following.

B.1 
Incremental Few-Shot Learning

Incremental few-shot learning (IFSL) 
[
271
, 
272
]
 focuses on the challenge of learning new categories with limited labeled data while retaining knowledge about previously learned categories.
In this scenario, a standard classification network has previously undergone training to recognize a predefined set of base classes. After that, the focus is on incorporating additional novel classes, each accompanied by only a small number of labeled examples. Subsequently, the model is tested on its classification performance, considering both the base and novel classes.

Existing Works:
 Gidaris et al. 
[
271
]
 propose the IFSL problem and an attention-based solution to mitigate the forgetting in IFSL.
The Attention Attractor Network, proposed by Ren et al. 
[
272
]
, is an alternative approach where the per-episode training objective during the incremental meta-learning stage is regulated using an attention mechanism to attend the set of base classes. In contrast to previous approaches that extract a fixed representation for each task, XtarNet 
[
273
]
 emphasizes the extraction of task-adaptive representations by combining novel and base features to enhance the adaptability of the representations.
Shi et al. 
[
274
]
 suggest putting more effort into the base classifier pretraining stage rather than the later few-shot learning stage. As a result, they propose to seek flat local minima of the base classifier training objective function and subsequently fine-tune the model parameters within that flat region when faced with new tasks.
In addition, C-FSCIL 
[
111
]
 incorporates a trainable fixed-size fully connected layer and a rewritable dynamically growing memory buffer to mitigate forgetting. This memory buffer can store a vector for each class encountered up to that point in the learning process.

B.2 
Continual Meta-Learning

The goal of continual meta-learning (CML) is to address the challenge of forgetting in non-stationary task distributions. Traditional meta-learning approaches typically focus on a single task distribution. However, CML extends this concept to handle a sequence of task distributions, denoted as 
P
1
⁢
(
𝒯
)
,
P
2
⁢
(
𝒯
)
,
⋯
,
P
N
⁢
(
𝒯
)
subscript
𝑃
1
𝒯
subscript
𝑃
2
𝒯
⋯
subscript
𝑃
𝑁
𝒯
P_{1}({\mathcal{T}}),P_{2}({\mathcal{T}}),\cdots,P_{N}({\mathcal{T}})
italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( caligraphic_T ) , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( caligraphic_T ) , ⋯ , italic_P start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( caligraphic_T )
.
In CML, the objective is to develop meta-learning algorithms that can effectively adapt and generalize to new task distributions as they arise over time. These task distributions can represent different environments, domains, or contexts. It aims to mitigate the forgetting of previously learned task distributions while efficiently adapting to new tasks.

Existing Works:

Online meta-learning (OML) 
[
275
]
 is a framework that assumes tasks arrive sequentially and aims to improve performance on future tasks. Jerfel et al. 
[
276
]
 extended the Model-Agnostic Meta-Learning 
[
204
]
 approach and utilized Dirichlet process mixtures to group similar training tasks together. However, this method is not scalable to large-scale non-stationary distributions due to the requirement of independent parameters for each component.
Yap et al. 
[
277
]
 proposed an approach to model the posterior distribution of meta-parameters using Laplace approximation 
[
68
]
. Zhang et al. 
[
278
]
 further extended this framework by employing a dynamical mixture model to learn the distribution of meta-parameters instead of a single distribution. Additionally, they used structural variational inference techniques to infer latent variables in the model.
Wang et al. 
[
279
, 
280
, 
281
]
 introduced a large-scale benchmark for sequential domain meta-learning. They proposed different settings, including supervised learning, imbalanced domains, and semi-supervised settings, to evaluate the performance of various methods in sequential domain meta-learning.