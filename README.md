
# ğŸ§  Research-It: Fully Local RAG for Research Papers

**Research-It** is a **fully local Retrieval-Augmented Generation (RAG) system** for research papers.  
It lets you **ingest PDFs, URLs, or directories of papers** â†’ build vector indexes â†’ and **chat with them** via a **Streamlit UI** backed by **Ollama LLMs**.  

Run everything **offline** on your own machine â€” no API keys, no cloud costs.  

---

## âœ¨ Features

- ğŸ“„ **Index Any Research Source**
  - Direct PDF files (local or from the web)
  - HTML pages (arXiv, NeurIPS, ResearchGate, etc.)
  - Entire folders of papers (`./data`)
- ğŸ§© **Chunking + Embeddings**
  - Smart text chunking with overlap
  - `sentence-transformers` for dense embeddings
  - `hnsw` index backend (FAISS-compatible, efficient)
- ğŸ” **Fast Local Retrieval**
  - LEANN (`leann`) powers vector search with pruning + quantization
- ğŸ¤– **Local LLM**
  - [Ollama](https://ollama.com) for running quantized models (`llama3.2:1b`, `3b`, etc.)
  - No GPU? Use CPU or small quantized models
- ğŸ¨ **Streamlit UI**
  - Upload a paper or enter a URL â†’ index instantly
  - Chat about your documents with sources and context shown
  - Sidebar lets you pick indexes, tweak chunk size, overlap, retrieval Top-K, context length, and model
- ğŸ”’ **Privacy by Design**
  - No data leaves your machine
  - No API keys, no cloud dependencies

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **[LEANN](https://github.com/leann-ai/leann)** â†’ embeddings, HNSW/CSR vector indexes
- **[sentence-transformers](https://www.sbert.net/)** â†’ embeddings (default: `facebook/contriever`)
- **[Ollama](https://ollama.com)** â†’ local quantized LLMs
- **Streamlit** â†’ simple and responsive chat UI
- **BeautifulSoup + readability** â†’ web text cleaning
- **PyMuPDF** â†’ PDF parsing
- **Typer** â†’ CLI utilities

---

## ğŸš€ Setup & Installation

We provide a **one-command setup script** for Linux/macOS and Windows.  
It installs Python deps, Ollama, pulls models, and sets up project folders.

### Linux / macOS

```bash
git clone https://github.com/spice14/research-it.git
cd research-it

# Run setup
bash scripts/setup.sh

# Activate virtualenv
source .venv/bin/activate

# Launch the app
streamlit run app_streamlit.py
````

### Windows (PowerShell)

```powershell
git clone https://github.com/spice14/research-it.git
cd research-it

# First time only
Set-ExecutionPolicy -Scope CurrentUser Bypass -Force

# Run setup
.\scripts\setup.ps1

# Activate venv
.\.venv\Scripts\Activate.ps1

# Launch the app
streamlit run app_streamlit.py

---

## ğŸ“š Usage

### 1. Build an Index (CLI)

* From a PDF:

```bash
python -m src.index.build_any \
  --index-path ./indexes/my-paper.leann \
  --file ~/Downloads/some_paper.pdf
```

* From a URL:

```bash
python -m src.index.build_any \
  --index-path ./indexes/my-paper.leann \
  --url "https://arxiv.org/pdf/2307.09218v3.pdf"
```

* From a folder of papers:

```bash
python -m src.index.build_any \
  --index-path ./indexes/my-library.leann \
  --dir ./data
```

This generates:

```
./indexes/<name>.index
./indexes/<name>.meta.json
./indexes/<name>.passages.idx
./indexes/<name>.passages.jsonl
```

---

### 2. Chat via Streamlit UI

Launch the app:

```bash
streamlit run app_streamlit.py
```

Features in the sidebar:

* Pick an existing index or type a path
* Build a new index from URL or PDF upload
* Select Ollama model (`llama3.2:1b` recommended for small GPUs)
* Adjust retrieval params (chunk size, overlap, Top-K, num\_ctx)

Ask questions in the chat box â€” youâ€™ll get an answer with **retrieved sources** shown below.

---

## âš¡ Performance Tips

* **Low VRAM (â‰¤4GB GPU)?**

  * Use `llama3.2:1b` or another quantized small model
  * Reduce `num_ctx` (e.g., 1024â€“1536)
  * Lower `Top-K` (3â€“4)
* **No GPU?**

  * Ollama runs CPU-only models too, just slower
* **Avoid CUDA OOM**:
  Export before running:

  ```bash
  export LEANN_EMBED_BATCH=4
  export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb=64,expandable_segments:True"
  ```

---

## ğŸ“¦ Project Layout

```
research-it/
â”œâ”€â”€ app_streamlit.py       # Streamlit UI
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh           # Linux/macOS setup
â”‚   â””â”€â”€ setup.ps1          # Windows setup
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index/             # build_any, build_from_url, etc.
â”‚   â”œâ”€â”€ ingest/            # PDF, HTML, file loaders
â”‚   â””â”€â”€ rag/               # CLI chat pipeline
â”œâ”€â”€ indexes/               # saved vector indexes
â”œâ”€â”€ data/                  # cleaned paper text dumps
â””â”€â”€ uploads/               # optional file uploads
```

---

## ğŸ”® Roadmap

* [ ] Support more embedding backends (bge-m3, InstructorXL, etc.)
* [ ] Add incremental indexing / updates
* [ ] Hybrid retrieval (BM25 + dense)
* [ ] Docker image for one-shot deploy
* [ ] Offline model bundling for air-gapped use

---

## â¤ï¸ Credits

* [LEANN](https://github.com/leann-ai/leann) â€” blazing fast vector store
* [Ollama](https://ollama.com) â€” local quantized LLMs
* [SentenceTransformers](https://www.sbert.net/) â€” embeddings
* [Streamlit](https://streamlit.io) â€” UI
* Inspired by the need to read and **truly understand papers locally** âœï¸
